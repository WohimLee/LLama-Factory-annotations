Metadata-Version: 2.1
Name: llamafactory
Version: 0.8.4.dev0
Summary: Easy-to-use LLM fine-tuning framework
Home-page: https://github.com/hiyouga/LLaMA-Factory
Author: hiyouga
Author-email: hiyouga@buaa.edu.cn
License: Apache 2.0 License
Keywords: LLaMA,BLOOM,Falcon,LLM,ChatGPT,transformer,pytorch,deep learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: transformers>=4.41.2
Requires-Dist: datasets>=2.16.0
Requires-Dist: accelerate>=0.30.1
Requires-Dist: peft>=0.11.1
Requires-Dist: trl>=0.8.6
Requires-Dist: gradio>=4.0.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: scipy
Requires-Dist: einops
Requires-Dist: sentencepiece
Requires-Dist: tiktoken
Requires-Dist: protobuf
Requires-Dist: uvicorn
Requires-Dist: pydantic
Requires-Dist: fastapi
Requires-Dist: sse-starlette
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: fire
Requires-Dist: packaging
Requires-Dist: pyyaml
Requires-Dist: numpy<2.0.0
Provides-Extra: torch
Requires-Dist: torch>=1.13.1; extra == "torch"
Provides-Extra: torch-npu
Requires-Dist: torch==2.1.0; extra == "torch-npu"
Requires-Dist: torch-npu==2.1.0.post3; extra == "torch-npu"
Requires-Dist: decorator; extra == "torch-npu"
Provides-Extra: metrics
Requires-Dist: nltk; extra == "metrics"
Requires-Dist: jieba; extra == "metrics"
Requires-Dist: rouge-chinese; extra == "metrics"
Provides-Extra: deepspeed
Requires-Dist: deepspeed>=0.10.0; extra == "deepspeed"
Provides-Extra: bitsandbytes
Requires-Dist: bitsandbytes>=0.39.0; extra == "bitsandbytes"
Provides-Extra: hqq
Requires-Dist: hqq; extra == "hqq"
Provides-Extra: eetq
Requires-Dist: eetq; extra == "eetq"
Provides-Extra: gptq
Requires-Dist: optimum>=1.17.0; extra == "gptq"
Requires-Dist: auto-gptq>=0.5.0; extra == "gptq"
Provides-Extra: awq
Requires-Dist: autoawq; extra == "awq"
Provides-Extra: aqlm
Requires-Dist: aqlm[gpu]>=1.1.0; extra == "aqlm"
Provides-Extra: vllm
Requires-Dist: vllm>=0.4.3; extra == "vllm"
Provides-Extra: galore
Requires-Dist: galore-torch; extra == "galore"
Provides-Extra: badam
Requires-Dist: badam>=1.2.1; extra == "badam"
Provides-Extra: qwen
Requires-Dist: transformers_stream_generator; extra == "qwen"
Provides-Extra: modelscope
Requires-Dist: modelscope; extra == "modelscope"
Provides-Extra: dev
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pytest; extra == "dev"

# LLaMA-Factory 调试版本

官方原版仓库: https://github.com/hiyouga/LLaMA-Factory

## 调试
所有调试开始前修改一下这个文件：
- src/llamafactory/hparams/parser.py

```py
def _parse_train_args(args: Optional[Dict[str, Any]] = None): #  -> _TRAIN_CLS
    parser = HfArgumentParser(_TRAIN_ARGS)
    parser.add_argument('--local-rank', type=int, default=0, help="Local rank for distributed training") # 添加这行
    return _parse_args(parser, args)
```
### 1 调试 cli.py
>命令
```sh
FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft_ds3.yaml
```

>launch.json
```json
{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    
    "version": "0.2.0",
    "configurations": [
        
        {
            "name": "Python: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/run-debug.py",
            "args": [
            ],
            "cwd": "${workspaceFolder}",
            "env": {
                "FORCE_TORCHRUN": "true",
                "PATH": "/opt/miniconda/envs/llama39/bin:${env:PATH}" // which torchrun 找到的路径, 否则 process = subprocess.run 找不到命令会报错
            },
            "console": "integratedTerminal",
            "justMyCode": false // 允许调试库代码
        }
    ]
}
```

>run-debug.py
```py
# 用来调试 llamafactory-cli 命令, 
# 也就是 src/llamafactory/cli.py 里面的 main 函数

import sys
from llamafactory.cli import main

sys.argv = ["llamafactory-cli", "train", "examples/train_full/llama3_full_sft_ds3.yaml"]

if __name__ == "__main__":
    main()
```

### 2 调试 launcher.py
>命令
- 从 1 获得
```sh
torchrun --nnodes 1 --node_rank 0 --nproc_per_node 1 \
  --master_addr 127.0.0.1 --master_port 20570 \
  /root/datav/nlp/LLama-Factory-annotations/src/llamafactory/launcher.py \
  examples/train_full/llama3_full_sft_ds3.yaml
```

>launch.json
- 使用 `which torchrun` 找到 torchrun 路径
- 然后直接把断点打在 launcher.py 里面就可以了
```json
{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    
    "version": "0.2.0",
    "configurations": [
        
        {
            "name": "Python: Current File",
            "type": "debugpy",
            "request": "launch",
            // "module": "llamafactory.cli",
            "program": "/opt/miniconda/envs/llama39/bin/torchrun",  // torchrun 的完整路径
            "args": [
                "--nnodes", "1",
                "--node_rank", "0",
                "--nproc_per_node", "1",
                "--master_addr", "127.0.0.1",
                "--master_port", "20570",
                "/root/datav/nlp/LLama-Factory-annotations/src/llamafactory/launcher.py",  // 脚本路径
                "examples/train_full/llama3_full_sft_ds3.yaml" 
            ],
            "cwd": "${workspaceFolder}",
            "env": {
                // "FORCE_TORCHRUN": "true",
                "PATH": "/opt/miniconda/envs/llama39/bin:${env:PATH}" 
            },
            "subProcess": true,
            "console": "integratedTerminal",
            "justMyCode": false // 允许调试库代码
        }
    ]
}
```


### 3 调试 
